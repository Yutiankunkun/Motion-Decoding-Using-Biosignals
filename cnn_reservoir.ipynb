{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import networkx as nx\n",
    "import glob\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, seq_length, is_train, transform=None):\n",
    "        self.transform = transform\n",
    "        self.seqs = []\n",
    "        self.seq_labels = []\n",
    "        self.class_names = os.listdir(root)\n",
    "        self.class_names.sort()\n",
    "        self.numof_classes = len(self.class_names)\n",
    "        self.seq_length = seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        for (i,x) in enumerate(self.class_names):\n",
    "            temp = glob.glob(os.path.join(root, x, '*'))\n",
    "            temp.sort()\n",
    "            self.seq_labels.extend([i]*len(temp))\n",
    "            for t in temp:\n",
    "                df = pd.read_csv(t, header=None)\n",
    "                tensor = preprocess(df)\n",
    "                self.seqs.append(tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        if self.transform is not None:\n",
    "            seq = self.transform(seq, is_train=self.is_train, seq_length=self.seq_length)\n",
    "        return {'seq':seq, 'label':self.seq_labels[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "def preprocess(df: pd.DataFrame)->np.ndarray:\n",
    "    mat = df.T.values\n",
    "    mat = standardization(mat, axis=1)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def standardization(a, axis=None, ddof=0):\n",
    "    a_mean = a.mean(axis=axis, keepdims=True)\n",
    "    a_std = a.std(axis=axis, keepdims=True, ddof=ddof)\n",
    "    a_std[np.where(a_std==0)] = 1\n",
    "\n",
    "    return (a - a_mean) / a_std\n",
    "\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    data_noisy = data + noise\n",
    "\n",
    "    return data_noisy.astype(np.float32)\n",
    "\n",
    "def time_shift(data, shift):\n",
    "    data_shifted = np.roll(data, shift)\n",
    "\n",
    "    return data_shifted\n",
    "\n",
    "def transform(array, is_train, seq_length):\n",
    "    if is_train:\n",
    "        _, n = array.shape\n",
    "        s = random.randint(0, n-seq_length)\n",
    "        ts = array[:,s:s+seq_length]\n",
    "        ts = add_noise(ts).astype(np.float32)\n",
    "        if random.randint(0,1):\n",
    "            ts_r = ts[:,::-1].copy()\n",
    "            return ts_r\n",
    "        return ts\n",
    "    else:\n",
    "        ts = array[:,:seq_length].astype(np.float32)\n",
    "        return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "\n",
    "train_datasets = []\n",
    "train_ids = ['subject0', 'subject1', 'subject2', 'subject3']\n",
    "\n",
    "for train_id in train_ids:\n",
    "    train_dir = os.path.join('Motion Decoding Using Biosignals', 'dataset', 'train', train_id)\n",
    "    dataset = SeqDataset(root=train_dir, seq_length=250, is_train=True, transform=transform)\n",
    "    train_datasets.append(dataset)\n",
    "\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_dir = os.path.join('Motion Decoding Using Biosignals', 'dataset', 'val', 'subject4')\n",
    "\n",
    "test_dataset = SeqDataset(root=test_dir, seq_length=250, is_train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling cnn_reservoir\n",
    "\n",
    "class CNN_RCLayer(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, spectral_radius=0.9, sparsity=0.1):\n",
    "        super(CNN_RCLayer, self).__init__()\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.Win = torch.empty(reservoir_size, input_size).uniform_(-1.0, 1.0)\n",
    "\n",
    "        self.W = torch.empty(reservoir_size, reservoir_size).uniform_(-1.0, 1.0)\n",
    "        mask = torch.rand_like(self.W) < sparsity\n",
    "        self.W = self.W * mask\n",
    "\n",
    "        eigvals = torch.linalg.eigvals(self.W)\n",
    "        max_eigval = torch.max(torch.abs(eigvals))\n",
    "        self.W = self.W * (spectral_radius / max_eigval)\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        self.Win = self.Win.to(device)\n",
    "        self.W = self.W.to(device)\n",
    "        self.state = torch.zeros(batch_size, self.reservoir_size, dtype=torch.float32).to(device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            u = x[:, t, :] \n",
    "            new_state = torch.tanh(torch.matmul(u, self.Win.T) + torch.matmul(self.state, self.W.T))\n",
    "            self.state = new_state\n",
    "            outputs.append(self.state.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class CNN_Reservoir(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes, reservoir_size=400):\n",
    "        super(CNN_Reservoir, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=2)        \n",
    "        #self.conv4 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=2)   \n",
    "        #self.conv5 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=2)   \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        #self.bn4 = nn.BatchNorm1d(128)\n",
    "        #self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(50)\n",
    "\n",
    "        self.reservoir = CNN_RCLayer(input_size=256, reservoir_size=reservoir_size)\n",
    "        self.fc = nn.Linear(reservoir_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        #x = self.relu(self.bn4(self.conv4(x)))\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        #x = self.relu(self.bn5(self.conv5(x)))\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        reservoir_output = self.reservoir(x) \n",
    "        out = reservoir_output[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "num_channels = 72\n",
    "num_classes = 3\n",
    "\n",
    "model = CNN_Reservoir(num_channels, num_classes)\n",
    "\n",
    "input_data = torch.randn(32, num_channels, 300)\n",
    "output_data = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 24.69%  Test Loss: 2.099197 \n",
      "\n",
      "Validation Accuracy: 32.01%  Test Loss: 2.063730 \n",
      "\n",
      "Validation Accuracy: 33.05%  Test Loss: 2.210656 \n",
      "\n",
      "Validation Accuracy: 29.71%  Test Loss: 2.907683 \n",
      "\n",
      "Validation Accuracy: 31.17%  Test Loss: 3.271265 \n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " backside_kickturn       0.39      0.27      0.32       123\n",
      "frontside_kickturn       0.12      0.17      0.14       115\n",
      "           pumping       0.42      0.40      0.41       240\n",
      "\n",
      "          accuracy                           0.31       478\n",
      "         macro avg       0.31      0.28      0.29       478\n",
      "      weighted avg       0.34      0.31      0.32       478\n",
      "\n",
      "Final Acccuracy: 0.3117154811715481\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch, iteration):\n",
    "\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for sample_batched in train_loader:\n",
    "        data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "    #    if iteration % log_interval == 0:\n",
    "    #        print('Train Accracy: {3:5.2f}%  train_loss: {2:.6f} \\n'.format(epoch, iteration, loss.item(), 100.*correct/float(len(sample_batched['label']))))\n",
    "            \n",
    "    return iteration\n",
    "\n",
    "def val(model, device, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_loader:\n",
    "            data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= float(len(test_loader.dataset))\n",
    "    correct /= float(len(test_loader.dataset))\n",
    "    \n",
    "    print('Validation Accuracy: {0:.2f}%  Test Loss: {1:.6f} \\n'.format(100. * correct, test_loss))\n",
    "\n",
    "    return test_loss, 100. * correct\n",
    "\n",
    "def evaluate(model, device, test_loader):\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_loader:\n",
    "            data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = [test_loader.dataset.class_names[i] for i in list(output.max(1)[1].cpu().detach().numpy())]\n",
    "            preds += pred\n",
    "            true = [test_loader.dataset.class_names[i] for i in list(target.cpu().detach().numpy())]\n",
    "            trues += true\n",
    "\n",
    "    labels = test_loader.dataset.class_names\n",
    "\n",
    "    cm = confusion_matrix(trues, preds, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    cr = classification_report(trues, preds, target_names=labels)\n",
    "    print(cr)\n",
    "    correct = 0\n",
    "\n",
    "    for pred, true in zip(preds, trues):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "            \n",
    "    df = pd.DataFrame({'pred': preds, 'true': trues})\n",
    "\n",
    "    return correct/len(trues), df\n",
    "\n",
    "def train_evaluate(train_loader, test_loader, log_interval, num_epoches, seq_length, transform=None, num_channels=72, num_classes=3):\n",
    "\n",
    "    model = CNN_Reservoir(num_channels=num_channels, num_classes=num_classes)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    iteration = 0\n",
    "\n",
    "    for epoch in range(1, 1+num_epoches):\n",
    "        iteration = train(log_interval, model, device, train_loader, optimizer, epoch, iteration)\n",
    "        if epoch%10==0:\n",
    "            test_loss, test_acc = val(model, device, test_loader)\n",
    "    acc, df = evaluate(model, device, test_loader)\n",
    "\n",
    "    print(f'Final Acccuracy: {acc}')\n",
    "    return model\n",
    "\n",
    "log_interval = 1000\n",
    "num_epoches = 50\n",
    "seq_length = 250\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "model = train_evaluate(train_loader, test_loader, log_interval, num_epoches, seq_length, transform)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
