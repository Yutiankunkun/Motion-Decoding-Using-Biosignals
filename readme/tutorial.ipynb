{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pymatreader import read_mat\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_mat(os.path.join('.', 'train', 'subject0', 'train2.mat'))\n",
    "print(data.keys()) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mne_data(data):\n",
    "    # MNEのチャンネル情報の設定\n",
    "    ch_names = [c.replace(' ', '') for c in data['ch_labels']]  # チャンネル名を取得\n",
    "    ch_types = ['eeg'] * len(ch_names)  # チャンネルタイプ（全てEEGと仮定）\n",
    "\n",
    "    # チャンネル情報を組み立てる\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=500, ch_types=ch_types) # type: ignore\n",
    "\n",
    "    # RawArrayオブジェクトの作成\n",
    "    raw = mne.io.RawArray(data['data']*1e-6, info) # Vに変換\n",
    "    raw.set_montage(mne.channels.make_standard_montage('standard_1020'))\n",
    "\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = make_mne_data(data)\n",
    "print(raw)\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot(duration=5, n_channels=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.DataFrame(data['event']).astype({'type': int, 'init_index':int}) # type: ignore\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['init_time'] = (events['init_time']*500).astype(int) # 2ms間隔のインデックスに変換\n",
    "events = events.rename(columns={'init_time': 'id', 'init_index':'test', 'type':'event_id'})[['id', 'test', 'event_id']]\n",
    "event_dict = {\n",
    "    'led/frontside_kickturn': 11,\n",
    "    'led/backside_kickturn': 12,\n",
    "    'led/pumping': 13,\n",
    "    'laser/frontside_kickturn': 21,\n",
    "    'laser/backside_kickturn': 22,\n",
    "    'laser/pumping': 23\n",
    "} # トリックの種別の対応付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_events(events, event_id=event_dict, sfreq=500) # サンプルレートは500Hzなので, sfreq=500に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_mat(os.path.join('.', 'test', 'subject0.mat'))\n",
    "print(test_data.keys()) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data['data'].shape, test_data['ch_labels'].shape) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(src_dir, dst_dir, subject_id):\n",
    "    print(subject_id)\n",
    "    # split to train and val\n",
    "    os.makedirs(os.path.join(dst_dir, 'train', subject_id), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dst_dir, 'val', subject_id), exist_ok=True)\n",
    "    labels = {\n",
    "        '11': 'frontside_kickturn',\n",
    "        '12': 'backside_kickturn',\n",
    "        '13': 'pumping',\n",
    "        '21': 'frontside_kickturn',\n",
    "        '22': 'backside_kickturn',\n",
    "        '23': 'pumping'\n",
    "    }\n",
    "    counts = {'frontside_kickturn':0, 'backside_kickturn':0, 'pumping':0}\n",
    "    for fname in os.listdir(os.path.join(src_dir, 'train', subject_id)):\n",
    "        data = read_mat(os.path.join(src_dir, 'train', subject_id, fname))\n",
    "        event = pd.DataFrame(data['event'])[['init_time', 'type']] # type: ignore\n",
    "        ts = pd.DataFrame(np.concatenate([np.array([data['times']]), data['data']]).T, columns=['Time']+list(data['ch_labels'])) # type: ignore\n",
    "        for i, d in event.iterrows():\n",
    "            it = d['init_time']+0.2\n",
    "            et = d['init_time']+0.7\n",
    "            event_type = str(int(d['type']))\n",
    "            ts_seg = ts[(ts['Time']>=it*1e3)&(ts['Time']<=et*1e3)]\n",
    "\n",
    "            if fname!='train3.mat':\n",
    "                if not os.path.exists(os.path.join(dst_dir, 'train', subject_id, labels[event_type])):\n",
    "                    os.makedirs(os.path.join(dst_dir, 'train', subject_id, labels[event_type]), exist_ok=True)\n",
    "                del ts_seg['Time']\n",
    "                ts_seg.to_csv(os.path.join(dst_dir, 'train', subject_id, labels[event_type], '{:03d}.csv'.format(counts[labels[event_type]])), index=False, header=False)\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(dst_dir, 'val', subject_id, labels[event_type])):\n",
    "                    os.makedirs(os.path.join(dst_dir, 'val', subject_id, labels[event_type]), exist_ok=True)\n",
    "                del ts_seg['Time']\n",
    "                ts_seg.to_csv(os.path.join(dst_dir, 'val', subject_id, labels[event_type], '{:03d}.csv'.format(counts[labels[event_type]])), index=False, header=False)\n",
    "\n",
    "\n",
    "            counts[labels[event_type]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '.'\n",
    "dst_dir = 'test_modeling'\n",
    "subject_ids = ['subject0', 'subject1', 'subject2', 'subject3', 'subject4']\n",
    "for subject_id in subject_ids:\n",
    "    make_data(src_dir=src_dir, dst_dir=dst_dir, subject_id=subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, root, seq_length, is_train, transform=None):\n",
    "        self.transform = transform\n",
    "        self.seqs = []\n",
    "        self.seq_labels = []\n",
    "        self.class_names = os.listdir(root)\n",
    "        self.class_names.sort()\n",
    "        self.numof_classes = len(self.class_names)\n",
    "        self.seq_length = seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        for (i,x) in enumerate(self.class_names):\n",
    "            temp = glob.glob(os.path.join(root, x, '*'))\n",
    "            temp.sort()\n",
    "            self.seq_labels.extend([i]*len(temp))\n",
    "            for t in temp:\n",
    "                df = pd.read_csv(t, header=None)\n",
    "                tensor = preprocess(df)\n",
    "                self.seqs.append(tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        if self.transform is not None:\n",
    "            seq = self.transform(seq, is_train=self.is_train, seq_length=self.seq_length)\n",
    "        return {'seq':seq, 'label':self.seq_labels[index]}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame)->np.ndarray:\n",
    "    # transpose\n",
    "    mat = df.T.values\n",
    "\n",
    "    # standerization\n",
    "    mat = standardization(mat, axis=1)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def standardization(a, axis=None, ddof=0):\n",
    "    a_mean = a.mean(axis=axis, keepdims=True)\n",
    "    a_std = a.std(axis=axis, keepdims=True, ddof=ddof)\n",
    "    a_std[np.where(a_std==0)] = 1\n",
    "\n",
    "    return (a - a_mean) / a_std\n",
    "\n",
    "\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    data_noisy = data + noise\n",
    "    return data_noisy.astype(np.float32)\n",
    "\n",
    "\n",
    "def time_shift(data, shift):\n",
    "    data_shifted = np.roll(data, shift)\n",
    "    return data_shifted\n",
    "\n",
    "\n",
    "def transform(array, is_train, seq_length):\n",
    "    if is_train:\n",
    "        _, n = array.shape\n",
    "        s = random.randint(0, n-seq_length)\n",
    "        ts = array[:,s:s+seq_length]\n",
    "        ts = add_noise(ts).astype(np.float32)\n",
    "        if random.randint(0,1):\n",
    "            ts_r = ts[:,::-1].copy()\n",
    "            return ts_r\n",
    "        return ts\n",
    "    else:\n",
    "        ts = array[:,:seq_length].astype(np.float32)\n",
    "        return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "train_dir = os.path.join('test_modeling', 'train', subject_id)\n",
    "dataset = SeqDataset(root=train_dir, seq_length=250, is_train=True, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.class_names)\n",
    "for i, mini_batch in enumerate(data_loader):\n",
    "    print(mini_batch['seq'].shape, mini_batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1DBN(torch.nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(Net1DBN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(num_channels, 128, kernel_size=3, stride=1)\n",
    "        self.conv2 = torch.nn.Conv1d(128, 128, kernel_size=3, stride=1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 128, kernel_size=3, stride=1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, 128, kernel_size=3, stride=1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(128)\n",
    "        self.maxpool = torch.nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.gap = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.squeeze(2)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 72  # チャンネル数\n",
    "num_classes = 3    # 判別するトリックの種別数\n",
    "model = Net1DBN(num_channels, num_classes)\n",
    "in_data = torch.randn(8, num_channels, 300)\n",
    "out_data = model(in_data)\n",
    "print(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch, iteration):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for sample_batched in train_loader:\n",
    "        data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iteration += 1\n",
    "        if iteration % log_interval == 0:\n",
    "            sys.stdout.write('\\repoch:{0:>3} iteration:{1:>6} train_loss: {2:.6f} train_accracy: {3:5.2f}%'.format(\n",
    "                            epoch, iteration, loss.item(), 100.*correct/float(len(sample_batched['label']))))\n",
    "            sys.stdout.flush()\n",
    "    return iteration\n",
    "\n",
    "\n",
    "def val(model, device, test_loader):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_loader:\n",
    "            data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= float(len(test_loader.dataset))\n",
    "    correct /= float(len(test_loader.dataset))\n",
    "    print('\\n  Validation: Accuracy: {0:.2f}%  test_loss: {1:.6f}'.format(100. * correct, test_loss))\n",
    "    return test_loss, 100. * correct\n",
    "\n",
    "\n",
    "def evaluate(model, device, test_loader):\n",
    "    preds = []\n",
    "    trues = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_loader:\n",
    "            data, target = sample_batched['seq'].to(device), sample_batched['label'].to(device)\n",
    "            output = model(data)\n",
    "            pred = [test_loader.dataset.class_names[i] for i in list(output.max(1)[1].cpu().detach().numpy())]\n",
    "            preds += pred\n",
    "            true = [test_loader.dataset.class_names[i] for i in list(target.cpu().detach().numpy())]\n",
    "            trues += true\n",
    "    labels = test_loader.dataset.class_names\n",
    "    cm = confusion_matrix(trues, preds, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    cr = classification_report(trues, preds, target_names=labels)\n",
    "    print(cr)\n",
    "    correct = 0\n",
    "    for pred, true in zip(preds, trues):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "    df = pd.DataFrame({'pred': preds, 'true': trues})\n",
    "\n",
    "    return correct/len(trues), df\n",
    "\n",
    "\n",
    "def train_evaluate(train_dir, val_dir, log_interval, num_epoches, seq_length, transform=None, num_channels=72, num_classes = 3):\n",
    "    model = Net1DBN(num_channels=num_channels, num_classes=num_classes)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(SeqDataset(root=train_dir, seq_length=seq_length, is_train=True, transform=transform), batch_size=20, shuffle=True) # type: ignore\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    val_loader = torch.utils.data.DataLoader(SeqDataset(root=val_dir, seq_length=seq_length, is_train=False, transform=transform), batch_size=20, shuffle=False) # type: ignore\n",
    "    iteration = 0\n",
    "    for epoch in range(1, 1+num_epoches):\n",
    "        iteration = train(log_interval, model, device, train_loader, optimizer, epoch, iteration)\n",
    "        if epoch%10==0:\n",
    "            test_loss, test_acc = val(model, device, val_loader)\n",
    "    acc, df = evaluate(model, device, val_loader)\n",
    "    print(acc)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 5000\n",
    "num_epoches = 100\n",
    "seq_length = 250\n",
    "models = {}\n",
    "for subject_id in subject_ids:\n",
    "    train_dir = os.path.join('test_modeling', 'train', subject_id)\n",
    "    val_dir = os.path.join('test_modeling', 'val', subject_id)\n",
    "    model = train_evaluate(train_dir, val_dir, log_interval, num_epoches, seq_length, transform)\n",
    "    models[subject_id] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_pred(src_dir, root_dir, subject_ids, models, seq_length, transform, device):\n",
    "    predictions = {}\n",
    "    for subject_id in subject_ids:\n",
    "        train_dir =os.path.join(root_dir, subject_id)\n",
    "        class_names = os.listdir(train_dir)\n",
    "        class_names.sort()\n",
    "        data = read_mat(os.path.join(src_dir, 'test', '{}.mat'.format(subject_id)))\n",
    "        for i, ts in enumerate(data['data']): # type: ignore\n",
    "            tensor = torch.from_numpy(transform(standardization(ts, axis=1), is_train=False, seq_length=seq_length)).unsqueeze(0).to(device) # type: ignore\n",
    "            pred = models[subject_id](tensor)\n",
    "            _, output_index = pred.max(1)\n",
    "            pred = output_index.squeeze(0).cpu().detach().numpy()\n",
    "            predictions['{}_{:03d}'.format(subject_id, i)]=class_names[pred]\n",
    "    result = pd.Series(predictions)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '.'\n",
    "root_dir = os.path.join('test_modeling', 'train')\n",
    "subject_ids = ['subject0', 'subject1', 'subject2', 'subject3', 'subject4']\n",
    "seq_length = 250\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "result = output_pred(src_dir, root_dir, subject_ids, models, seq_length, transform, device)\n",
    "result.to_csv('submit.csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
